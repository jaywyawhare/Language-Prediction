{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nature, in the broadest sense, is the natural...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Nature\" can refer to the phenomena of the phy...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The study of nature is a large, if not the onl...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Although humans are part of nature, human acti...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1] The word nature is borrowed from the Old F...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Language\n",
       "0   Nature, in the broadest sense, is the natural...  English\n",
       "1  \"Nature\" can refer to the phenomena of the phy...  English\n",
       "2  The study of nature is a large, if not the onl...  English\n",
       "3  Although humans are part of nature, human acti...  English\n",
       "4  [1] The word nature is borrowed from the Old F...  English"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/Language.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['Text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['Text'])\n",
    "\n",
    "MAX_LEN = 200\n",
    "padded_sequences = pad_sequences(sequences, maxlen=MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=10000, embedding_dim=128)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        \n",
    "    def forward_once(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.relu(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.forward_once(x1)\n",
    "        x2 = self.forward_once(x2)\n",
    "        distance = torch.sqrt(torch.sum(torch.pow((x1 - x2), 2), dim=1))\n",
    "        return distance\n",
    "        \n",
    "class LanguageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        text1 = self.texts[index]\n",
    "        label1 = self.labels[index]\n",
    "        positive_indices = np.where(self.labels == label1)[0]\n",
    "        negative_indices = np.where(self.labels != label1)[0]\n",
    "        np.random.shuffle(positive_indices)\n",
    "        np.random.shuffle(negative_indices)\n",
    "        text2_positive = self.texts[positive_indices[0]]\n",
    "        text2_negative = self.texts[negative_indices[0]]\n",
    "        return (text1, text2_positive, 0), (text1, text2_negative, 1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "def train_siamese_network(train_loader, val_loader, model, criterion, optimizer, device, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        for i, (batch_pos, batch_neg) in enumerate(train_loader):\n",
    "            x1_pos, x2_pos, y_pos = batch_pos\n",
    "            x1_neg, x2_neg, y_neg = batch_neg\n",
    "            \n",
    "            x1 = torch.cat([x1_pos, x1_neg]).to(device)\n",
    "            x2 = torch.cat([x2_pos, x2_neg]).to(device)\n",
    "            y = torch.cat([y_pos, y_neg]).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(x1, x2)\n",
    "\n",
    "            loss = criterion(output, y)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss, val_acc = evaluate_siamese_network(val_loader, model, criterion, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} -- Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "def evaluate_siamese_network(val_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_pos, batch_neg in val_loader:\n",
    "            x1_pos, x2_pos, y_pos = batch_pos\n",
    "            x1_neg, x2_neg, y_neg = batch_neg\n",
    "            \n",
    "            x1 = torch.cat([x1_pos, x1_neg]).to(device)\n",
    "            x2 = torch.cat([x2_pos, x2_neg]).to(device)\n",
    "            y = torch.cat([y_pos, y_neg]).to(device)\n",
    "\n",
    "            outputs = model(x1, x2)\n",
    "            loss = criterion(outputs, y)\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_acc += torch.sum(preds == y).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc /= len(val_loader.dataset)\n",
    "\n",
    "    return val_loss, val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 12:34:43.145977: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-31 12:34:43.147932: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-31 12:34:43.148162: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (n00b): /proc/driver/nvidia/version does not exist\n",
      "2023-03-31 12:34:43.152857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 64)           3117248   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                33024     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,150,337\n",
      "Trainable params: 3,150,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=MAX_LEN))\n",
    "model.add(LSTM(units=64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the Language column\n",
    "le = LabelEncoder()\n",
    "df['Language'] = le.fit_transform(df['Language'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "259/259 [==============================] - 292s 1s/step - loss: -105.9026 - accuracy: 0.0518 - val_loss: -164.6836 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "259/259 [==============================] - 1415s 5s/step - loss: -236.0537 - accuracy: 0.0518 - val_loss: -276.6971 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "259/259 [==============================] - 213s 823ms/step - loss: -353.9709 - accuracy: 0.0518 - val_loss: -385.7858 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "259/259 [==============================] - 206s 795ms/step - loss: -469.9426 - accuracy: 0.0518 - val_loss: -493.5753 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "259/259 [==============================] - 202s 780ms/step - loss: -585.0032 - accuracy: 0.0518 - val_loss: -600.9504 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "259/259 [==============================] - 206s 795ms/step - loss: -699.5575 - accuracy: 0.0518 - val_loss: -707.9578 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "259/259 [==============================] - 239s 925ms/step - loss: -813.8812 - accuracy: 0.0518 - val_loss: -814.6128 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "259/259 [==============================] - 212s 820ms/step - loss: -930.6855 - accuracy: 0.0518 - val_loss: -926.4080 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "259/259 [==============================] - 192s 742ms/step - loss: -1048.8766 - accuracy: 0.0518 - val_loss: -1035.7112 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "259/259 [==============================] - 192s 740ms/step - loss: -1165.5159 - accuracy: 0.0518 - val_loss: -1144.4321 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2349181520>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_sequences, df['Language'], epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove all the special characters\n",
    "    text = re.sub(r'\\W', ' ', str(text)) \n",
    "    # remove numbers\n",
    "    text = re.sub(r'\\d', ' ', str(text))      \n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mcleaned_text\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mText\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(clean_text)\n\u001b[1;32m      2\u001b[0m df\u001b[39m.\u001b[39mtail()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df['cleaned_text'] = df['Text'].apply(clean_text)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X \u001b[39m=\u001b[39m df1[\u001b[39m'\u001b[39m\u001b[39mcleaned_text\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m y \u001b[39m=\u001b[39m df1[\u001b[39m'\u001b[39m\u001b[39mLanguage\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m le \u001b[39m=\u001b[39m LabelEncoder()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "X = df1['cleaned_text']\n",
    "y = df1['Language']\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find the number of Texts in each language\n",
    "# df.groupby('Language').count()\n",
    "\n",
    "# # ['English', 'Malayalam', 'Hindi', 'Tamil', 'Portugeese', 'French','Dutch', 'Spanish', 'Greek', 'Russian', 'Danish', 'Italian', 'Turkish', 'Sweedish', 'Arabic', 'German', 'Kannada']\n",
    "# English = df[df['Language'] == 'English']\n",
    "# Malayalam = df[df['Language'] == 'Malayalam']\n",
    "# Hindi = df[df['Language'] == 'Hindi']\n",
    "# Tamil = df[df['Language'] == 'Tamil']\n",
    "# Portugeese = df[df['Language'] == 'Portugeese']\n",
    "# French = df[df['Language'] == 'French']\n",
    "# Dutch = df[df['Language'] == 'Dutch']\n",
    "# Spanish = df[df['Language'] == 'Spanish']\n",
    "# Greek = df[df['Language'] == 'Greek']\n",
    "# Russian = df[df['Language'] == 'Russian']\n",
    "# Danish = df[df['Language'] == 'Danish']\n",
    "# Italian = df[df['Language'] == 'Italian']\n",
    "# Turkish = df[df['Language'] == 'Turkish']\n",
    "# Sweedish = df[df['Language'] == 'Sweedish']\n",
    "# Arabic = df[df['Language'] == 'Arabic']\n",
    "# German = df[df['Language'] == 'German']\n",
    "# Kannada = df[df['Language'] == 'Kannada']\n",
    "\n",
    "# sizeEnglish = len(English)\n",
    "# sizeMalayalam = len(Malayalam)\n",
    "# sizeHindi = len(Hindi)\n",
    "# sizeTamil = len(Tamil)\n",
    "# sizePortugeese = len(Portugeese)\n",
    "# sizeFrench = len(French)\n",
    "# sizeDutch = len(Dutch)\n",
    "# sizeSpanish = len(Spanish)\n",
    "# sizeGreek = len(Greek)\n",
    "# sizeRussian = len(Russian)\n",
    "# sizeDanish = len(Danish)\n",
    "# sizeItalian = len(Italian)\n",
    "# sizeTurkish = len(Turkish)\n",
    "# sizeSweedish = len(Sweedish)\n",
    "# sizeArabic = len(Arabic)\n",
    "# sizeGerman = len(German)\n",
    "# sizeKannada = len(Kannada)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # divide the df into df1 df2 and df3 with 3 equal parts of each language\n",
    "\n",
    "# import random\n",
    "# df1 = pd.DataFrame()\n",
    "# df2 = pd.DataFrame()\n",
    "# df3 = pd.DataFrame()\n",
    "\n",
    "# df1 = pd.concat([df1, English.sample(n = sizeEnglish//3, random_state = 1)])\n",
    "# df1 = pd.concat([df1, Malayalam.sample(n = sizeMalayalam//3, random_state = 1)])\n",
    "# df1 = pd.concat([df1, Hindi.sample(n = sizeHindi//3, random_state = 1)])\n",
    "# df1 = pd.concat([df1, Tamil.sample(n = sizeTamil//3, random_state = 1)])\n",
    "# df1 = pd.concat([df1, Portugeese.sample(n = sizePortugeese//3, random_state = 1)])\n",
    "# df1 = pd.concat([df1, French.sample(n = sizeFrench//3, random_state = 1)])\n",
    "# df1 = pd.concat([df1, Dutch.sample(n = sizeDutch//3, random_state = 1)])\n",
    "# df1 = pd.concat([df1, Spanish.sample(n = sizeSpanish//3, random_state = 1)])\n",
    "# df1 = pd.concat([df1, Greek.sample(n = sizeGreek//3, random_state = 1)])\n",
    "# df1 = pd.concat([df1, Russian.sample(n = sizeRussian//3, random_state = 1)])\n",
    "# df1 = pd.concat([df1, Danish.sample(n = sizeDanish//3, random_state = 1)])\n",
    "# df1 = pd.concat([df1, Italian.sample(n = sizeItalian//3, random_state = 1)])\n",
    "# df1 = pd.concat([df1, Turkish.sample(n = sizeTurkish//3, random_state = 1)])\n",
    "# df1 = pd.concat([df1, Sweedish.sample(n = sizeSweedish//3, random_state = 1)])\n",
    "# df1 = pd.concat([df1, Arabic.sample(n = sizeArabic//3, random_state = 1)])\n",
    "# df1 = pd.concat([df1, German.sample(n = sizeGerman//3, random_state = 1)])\n",
    "# df1 = pd.concat([df1, Kannada.sample(n = sizeKannada//3, random_state = 1)])\n",
    "\n",
    "# df2 = pd.concat([df2, English.sample(n = sizeEnglish//3, random_state = 2)])\n",
    "# df2 = pd.concat([df2, Malayalam.sample(n = sizeMalayalam//3, random_state = 2)])\n",
    "# df2 = pd.concat([df2, Hindi.sample(n = sizeHindi//3, random_state = 2)])\n",
    "# df2 = pd.concat([df2, Tamil.sample(n = sizeTamil//3, random_state = 2)])\n",
    "# df2 = pd.concat([df2, Portugeese.sample(n = sizePortugeese//3, random_state = 2)])\n",
    "# df2 = pd.concat([df2, French.sample(n = sizeFrench//3, random_state = 2)])\n",
    "# df2 = pd.concat([df2, Dutch.sample(n = sizeDutch//3, random_state = 2)])\n",
    "# df2 = pd.concat([df2, Spanish.sample(n = sizeSpanish//3, random_state = 2)])\n",
    "# df2 = pd.concat([df2, Greek.sample(n = sizeGreek//3, random_state = 2)])\n",
    "# df2 = pd.concat([df2, Russian.sample(n = sizeRussian//3, random_state = 2)])\n",
    "# df2 = pd.concat([df2, Danish.sample(n = sizeDanish//3, random_state = 2)])\n",
    "# df2 = pd.concat([df2, Italian.sample(n = sizeItalian//3, random_state = 2)])\n",
    "# df2 = pd.concat([df2, Turkish.sample(n = sizeTurkish//3, random_state = 2)])\n",
    "# df2 = pd.concat([df2, Sweedish.sample(n = sizeSweedish//3, random_state = 2)])\n",
    "# df2 = pd.concat([df2, Arabic.sample(n = sizeArabic//3, random_state = 2)])\n",
    "# df2 = pd.concat([df2, German.sample(n = sizeGerman//3, random_state = 2)])\n",
    "# df2 = pd.concat([df2, Kannada.sample(n = sizeKannada//3, random_state = 2)])\n",
    "\n",
    "# df3 = pd.concat([df3, English.sample(n = sizeEnglish//3, random_state = 3)])\n",
    "# df3 = pd.concat([df3, Malayalam.sample(n = sizeMalayalam//3, random_state = 3)])\n",
    "# df3 = pd.concat([df3, Hindi.sample(n = sizeHindi//3, random_state = 3)])\n",
    "# df3 = pd.concat([df3, Tamil.sample(n = sizeTamil//3, random_state = 3)])\n",
    "# df3 = pd.concat([df3, Portugeese.sample(n = sizePortugeese//3, random_state = 3)])\n",
    "# df3 = pd.concat([df3, French.sample(n = sizeFrench//3, random_state = 3)])\n",
    "# df3 = pd.concat([df3, Dutch.sample(n = sizeDutch//3, random_state = 3)])\n",
    "# df3 = pd.concat([df3, Spanish.sample(n = sizeSpanish//3, random_state = 3)])\n",
    "# df3 = pd.concat([df3, Greek.sample(n = sizeGreek//3, random_state = 3)])\n",
    "# df3 = pd.concat([df3, Russian.sample(n = sizeRussian//3, random_state = 3)])\n",
    "# df3 = pd.concat([df3, Danish.sample(n = sizeDanish//3, random_state = 3)])\n",
    "# df3 = pd.concat([df3, Italian.sample(n = sizeItalian//3, random_state = 3)])\n",
    "# df3 = pd.concat([df3, Turkish.sample(n = sizeTurkish//3, random_state = 3)])\n",
    "# df3 = pd.concat([df3, Sweedish.sample(n = sizeSweedish//3, random_state = 3)])\n",
    "# df3 = pd.concat([df3, Arabic.sample(n = sizeArabic//3, random_state = 3)])\n",
    "# df3 = pd.concat([df3, German.sample(n = sizeGerman//3, random_state = 3)])\n",
    "# df3 = pd.concat([df3, Kannada.sample(n = sizeKannada//3, random_state = 3)])\n",
    "\n",
    "# # df1.to_csv('../data/df1.csv', index = False)\n",
    "# # df2.to_csv('../data/df2.csv', index = False)\n",
    "# # df3.to_csv('../data/df3.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2752, 19296) (688, 19296) (2752,) (688,)\n"
     ]
    }
   ],
   "source": [
    "# split df1 into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "    print('Precision:', precision_score(y_test, y_pred, average = 'weighted'))\n",
    "    print('Recall:', recall_score(y_test, y_pred, average = 'weighted'))\n",
    "    print('F1:', f1_score(y_test, y_pred, average = 'weighted'))\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    # model = SVC()\n",
    "    # eval(model, X_train, X_test, y_train, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb1a287cbc244efabee0ff391601134"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'FloatProgress' object has no attribute 'style'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m BartTokenizer, BartForSequenceClassification\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[39m=\u001b[39m BartTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mfacebook/bart-large-mnli\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m model \u001b[39m=\u001b[39m BartForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mfacebook/bart-large-mnli\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_bart_embeddings\u001b[39m(text):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:1760\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1758\u001b[0m             resolved_vocab_files[file_id] \u001b[39m=\u001b[39m download_url(file_path, proxies\u001b[39m=\u001b[39mproxies)\n\u001b[1;32m   1759\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1760\u001b[0m         resolved_vocab_files[file_id] \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m   1761\u001b[0m             pretrained_model_name_or_path,\n\u001b[1;32m   1762\u001b[0m             file_path,\n\u001b[1;32m   1763\u001b[0m             cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1764\u001b[0m             force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   1765\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1766\u001b[0m             resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   1767\u001b[0m             local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1768\u001b[0m             use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1769\u001b[0m             user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   1770\u001b[0m             revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1771\u001b[0m             subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   1772\u001b[0m             _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1773\u001b[0m             _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1774\u001b[0m             _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   1775\u001b[0m         )\n\u001b[1;32m   1776\u001b[0m         commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_vocab_files[file_id], commit_hash)\n\u001b[1;32m   1778\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(unresolved_files) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/utils/hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    406\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    410\u001b[0m         path_or_repo_id,\n\u001b[1;32m    411\u001b[0m         filename,\n\u001b[1;32m    412\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    413\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    414\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    415\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    416\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    417\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    418\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    419\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    421\u001b[0m     )\n\u001b[1;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m    424\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    425\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/huggingface_hub/utils/_validators.py:124\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    120\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(\n\u001b[1;32m    121\u001b[0m         fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[1;32m    122\u001b[0m     )\n\u001b[0;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/huggingface_hub/file_download.py:1242\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[39mwith\u001b[39;00m temp_file_manager() \u001b[39mas\u001b[39;00m temp_file:\n\u001b[1;32m   1240\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mdownloading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, temp_file\u001b[39m.\u001b[39mname)\n\u001b[0;32m-> 1242\u001b[0m     http_get(\n\u001b[1;32m   1243\u001b[0m         url_to_download,\n\u001b[1;32m   1244\u001b[0m         temp_file,\n\u001b[1;32m   1245\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1246\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[1;32m   1247\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1248\u001b[0m     )\n\u001b[1;32m   1250\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mstoring \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, blob_path)\n\u001b[1;32m   1251\u001b[0m _chmod_and_replace(temp_file\u001b[39m.\u001b[39mname, blob_path)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/huggingface_hub/file_download.py:487\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries)\u001b[0m\n\u001b[1;32m    485\u001b[0m content_length \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mContent-Length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    486\u001b[0m total \u001b[39m=\u001b[39m resume_size \u001b[39m+\u001b[39m \u001b[39mint\u001b[39m(content_length) \u001b[39mif\u001b[39;00m content_length \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 487\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[1;32m    488\u001b[0m     unit\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mB\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    489\u001b[0m     unit_scale\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    490\u001b[0m     total\u001b[39m=\u001b[39;49mtotal,\n\u001b[1;32m    491\u001b[0m     initial\u001b[39m=\u001b[39;49mresume_size,\n\u001b[1;32m    492\u001b[0m     desc\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mDownloading\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    493\u001b[0m     disable\u001b[39m=\u001b[39;49m\u001b[39mbool\u001b[39;49m(logger\u001b[39m.\u001b[39;49mgetEffectiveLevel() \u001b[39m==\u001b[39;49m logging\u001b[39m.\u001b[39;49mNOTSET),\n\u001b[1;32m    494\u001b[0m )\n\u001b[1;32m    495\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m):\n\u001b[1;32m    496\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/huggingface_hub/utils/tqdm.py:120\u001b[0m, in \u001b[0;36mtqdm.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m are_progress_bars_disabled():\n\u001b[1;32m    119\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdisable\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tqdm/notebook.py:236\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m     display(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer)\n\u001b[1;32m    235\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplay\n\u001b[0;32m--> 236\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolour \u001b[39m=\u001b[39m colour\n\u001b[1;32m    238\u001b[0m \u001b[39m# Print initial bar state\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisable:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tqdm/notebook.py:194\u001b[0m, in \u001b[0;36mtqdm_notebook.colour\u001b[0;34m(self, bar_color)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39m@colour\u001b[39m\u001b[39m.\u001b[39msetter\n\u001b[1;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcolour\u001b[39m(\u001b[39mself\u001b[39m, bar_color):\n\u001b[1;32m    193\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcontainer\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 194\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontainer\u001b[39m.\u001b[39;49mchildren[\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m]\u001b[39m.\u001b[39;49mstyle\u001b[39m.\u001b[39mbar_color \u001b[39m=\u001b[39m bar_color\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FloatProgress' object has no attribute 'style'"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "model = BartForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n",
    "\n",
    "def get_bart_embeddings(text):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    return last_hidden_state\n",
    "\n",
    "def create_siamese_model(embedding_dim):\n",
    "    # Define the two input layers\n",
    "    input_a = Input(shape=(embedding_dim,))\n",
    "    input_b = Input(shape=(embedding_dim,))\n",
    "\n",
    "    # Define the shared embedding layer\n",
    "    shared_embedding_layer = Sequential([\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(embedding_dim)\n",
    "    ])\n",
    "\n",
    "    # Compute the embeddings for the two inputs\n",
    "    embedding_a = shared_embedding_layer(input_a)\n",
    "    embedding_b = shared_embedding_layer(input_b)\n",
    "\n",
    "    # Define the lambda layer to compute the absolute difference between the two embeddings\n",
    "    difference_layer = Lambda(lambda x: K.abs(x[0] - x[1]))\n",
    "\n",
    "    # Compute the absolute difference between the two embeddings\n",
    "    difference = difference_layer([embedding_a, embedding_b])\n",
    "\n",
    "    # Define the output layer to predict if the two embeddings are similar or not\n",
    "    output_layer = Dense(1, activation='sigmoid')(difference)\n",
    "\n",
    "    # Define the siamese model with the two input layers and the output layer\n",
    "    siamese_model = Model(inputs=[input_a, input_b], outputs=output_layer)\n",
    "\n",
    "    return siamese_model\n",
    "\n",
    "# Get the BART embeddings for the text data\n",
    "X = df1['cleaned_text'].apply(get_bart_embeddings)\n",
    "X = np.array(X.tolist())\n",
    "\n",
    "# Reshape the embeddings to a 2D shape\n",
    "X = X.reshape(X.shape[0], -1)\n",
    "\n",
    "# Use siamese network to learn a mapping of the embeddings to a 17D space\n",
    "siamese_net = create_siamese_model(embedding_dim=X.shape[1])\n",
    "siamese_net.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy')\n",
    "siamese_net.fit([X, X], y, batch_size=32, epochs=10, verbose=1)\n",
    "\n",
    "# Use k-means clustering to cluster the embeddings into 17 groups\n",
    "kmeans = KMeans(n_clusters=17, n_init=10, max_iter=300)\n",
    "kmeans.fit(siamese_net.predict([X, X]))\n",
    "\n",
    "# Map each text to its corresponding cluster label\n",
    "df1['cluster'] = kmeans.labels_\n",
    "\n",
    "# Print the language for each cluster\n",
    "for i in range(17):\n",
    "    cluster_df = df1[df1['cluster'] == i]\n",
    "    language = cluster_df['Language'].value_counts().idxmax()\n",
    "    print(f\"Cluster {i}: {language}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m models \u001b[39m=\u001b[39m [LinearRegression(), RandomForestClassifier(), LogisticRegression(), DecisionTreeClassifier(), GaussianNB(), KNeighborsClassifier(), SVC(), XGBClassifier()]\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m models:\n\u001b[0;32m----> 3\u001b[0m     \u001b[39meval\u001b[39;49m(model, X_train, X_test, y_train, y_test)\n",
      "Cell \u001b[0;32mIn [10], line 4\u001b[0m, in \u001b[0;36meval\u001b[0;34m(model, X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m      3\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m----> 4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m, accuracy_score(y_test, y_pred))\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mPrecision:\u001b[39m\u001b[39m'\u001b[39m, precision_score(y_test, y_pred, average \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mweighted\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mRecall:\u001b[39m\u001b[39m'\u001b[39m, recall_score(y_test, y_pred, average \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mweighted\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:211\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \n\u001b[1;32m    147\u001b[0m \u001b[39mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[39m0.5\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[39m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m    212\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    213\u001b[0m \u001b[39mif\u001b[39;00m y_type\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mmultilabel\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     90\u001b[0m     y_type \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(y_type) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     94\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mClassification metrics can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt handle a mix of \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m targets\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     95\u001b[0m             type_true, type_pred\n\u001b[1;32m     96\u001b[0m         )\n\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     99\u001b[0m \u001b[39m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[1;32m    100\u001b[0m y_type \u001b[39m=\u001b[39m y_type\u001b[39m.\u001b[39mpop()\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and continuous targets"
     ]
    }
   ],
   "source": [
    "models = [LinearRegression(), RandomForestClassifier(), LogisticRegression(), DecisionTreeClassifier(), GaussianNB(), KNeighborsClassifier(), SVC(), XGBClassifier()]\n",
    "for model in models:\n",
    "    eval(model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f'{model}:')\n",
    "    print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "    print(f'Precision: {precision_score(y_test, y_pred, average=\"macro\")}')\n",
    "    print(f'Recall: {recall_score(y_test, y_pred, average=\"macro\")}')\n",
    "    print(f'F1: {f1_score(y_test, y_pred, average=\"macro\")}')\n",
    "    # use cm from sklearn.metrics\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    # greys \n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bert_embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbert_embedding\u001b[39;00m \u001b[39mimport\u001b[39;00m BertEmbedding\n\u001b[1;32m      4\u001b[0m bert_abstract \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39mWe introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39m Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[39m As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. \u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[39mBERT is conceptually simple and empirically powerful. \u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[39mIt obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4\u001b[39m\u001b[39m%\u001b[39m\u001b[39m (7.6\u001b[39m\u001b[39m% a\u001b[39;00m\u001b[39mbsolute improvement), MultiNLI accuracy to 86.7 (5.6\u001b[39m\u001b[39m% a\u001b[39;00m\u001b[39mbsolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5\u001b[39m\u001b[39m% a\u001b[39;00m\u001b[39mbsolute improvement), outperforming human performance by 2.0\u001b[39m\u001b[39m%\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      9\u001b[0m sentences \u001b[39m=\u001b[39m bert_abstract\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bert_embedding'"
     ]
    }
   ],
   "source": [
    "from bert_embedding import BertEmbedding\n",
    "\n",
    "\n",
    "bert_abstract = \"\"\"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.\n",
    " Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.\n",
    " As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. \n",
    "BERT is conceptually simple and empirically powerful. \n",
    "It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4% (7.6% absolute improvement), MultiNLI accuracy to 86.7 (5.6% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5% absolute improvement), outperforming human performance by 2.0%.\"\"\"\n",
    "sentences = bert_abstract.split('\\n')\n",
    "bert_embedding = BertEmbedding()\n",
    "result = bert_embedding(sentences)\n",
    "\n",
    "# number of input in given model\n",
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3761547776.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    mport os\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "mport os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "dataset = pd.read_csv('Language Detection.csv')\n",
    "\n",
    "# Preprocess the text data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(dataset['Text'])\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset = dataset[:train_size]\n",
    "val_dataset = dataset[train_size:]\n",
    "\n",
    "# contrastive learning\n",
    "# take embedding of each sentence and use principle component analysis to reduce dimensionality\n",
    "# plot the embeddings in 3D space and see if the clusters are well separated\n",
    "# use k-means clustering to cluster the embeddings into 17 groups\n",
    "\n",
    "\n",
    "class LanguageDetectionModel(keras.Model):\n",
    "    def __init__(self, num_classes=17):\n",
    "        super(LanguageDetectionModel, self).__init__()\n",
    "        self.embedding = layers.Embedding(input_dim=10000, output_dim=128)\n",
    "        self.gru = layers.GRU(32)\n",
    "        self.classifier = layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.gru(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Create the model\n",
    "model = LanguageDetectionModel()\n",
    "model.compile("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define a dataset for pairs of embeddings\n",
    "class EmbeddingPairsDataset(Dataset):\n",
    "    def __init__(self, pairs, labels):\n",
    "        self.pairs = pairs\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x1, x2, label = self.pairs[idx], self.labels[idx]\n",
    "        return x1, x2, label\n",
    "\n",
    "# create positive and negative pairs of embeddings for language detection\n",
    "def create_language_pairs(embeddings, labels):\n",
    "    pairs = []\n",
    "    pair_labels = []\n",
    "    for i in range(len(embeddings)):\n",
    "        for j in range(i+1, len(embeddings)):\n",
    "            if labels[i] == labels[j]:\n",
    "                pairs.append((embeddings[i], embeddings[j]))\n",
    "                pair_labels.append(1)\n",
    "            else:\n",
    "                pairs.append((embeddings[i], embeddings[j]))\n",
    "                pair_labels.append(0)\n",
    "    return pairs, pair_labels\n",
    "\n",
    "# define a Siamese network for contrastive learning\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim=768):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.fc1 = nn.Linear(embedding_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        \n",
    "    def forward_once(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc3(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc4(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.forward_once(x1)\n",
    "        x2 = self.forward_once(x2)\n",
    "        return x1, x2\n",
    "    \n",
    "# define a contrastive loss function\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        \n",
    "    def forward(self, x1, x2, label):\n",
    "        distance = nn.functional.pairwise_distance(x1, x2)\n",
    "        loss = torch.mean((label.float()) * torch.pow(distance, 2) + \n",
    "                          (1 - label.float()) * torch.pow(torch.clamp(self.margin - distance, min=0.0), 2))\n",
    "        return loss\n",
    "\n",
    "# create a function to train the Siamese network\n",
    "def train_siamese_network(siamese_network, train_loader, optimizer, criterion, device):\n",
    "    siamese_network.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data1, data2, label) in enumerate(train_loader):\n",
    "        data1, data2, label = data1.to(device), data2.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output1, output2 = siamese_network(data1, data2)\n",
    "        loss = criterion(output1, output2, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "# create a function to evaluate the Siamese network\n",
    "def evaluate_siamese_network(siamese_network, test_loader, criterion, device):\n",
    "    siamese_network.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data1, data2, label) in enumerate(test_loader):\n",
    "            data1, data2, label = data1.to(device), data2.to(device), label.to(device)\n",
    "            output1, output2 = siamese_network(data1, data2)\n",
    "            loss = criterion(output1, output2, label)\n",
    "            running_loss += loss.item()\n",
    "    return running_loss / len(test_loader)\n",
    "\n",
    "# create a function to plot the loss\n",
    "def plot_loss(train_loss, test_loss):\n",
    "    plt.plot(train_loss, label='train loss')\n",
    "    plt.plot(test_loss, label='test loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# create a function to plot the embeddings\n",
    "def plot_embeddings(embeddings, labels):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(len(embeddings)):\n",
    "        x, y = embeddings[i, 0], embeddings[i, 1]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(labels[i], xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "# create a function to plot the embeddings in 3D space\n",
    "def plot_embeddings_3d(embeddings, labels):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    for i in range(len(embeddings)):\n",
    "        x, y, z = embeddings[i, 0], embeddings[i, 1], embeddings[i, 2]\n",
    "        ax.scatter(x, y, z)\n",
    "        ax.text(x, y, z, labels[i])\n",
    "    plt.show()\n",
    "\n",
    "# create a function evaluate the Siamese network\n",
    "def evaluate_siamese_network(siamese_network, test_loader, criterion, device):\n",
    "    siamese_network.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data1, data2, label) in enumerate(test_loader):\n",
    "            data1, data2, label = data1.to(device), data2.to(device), label.to(device)\n",
    "            output1, output2 = siamese_network(data1, data2)\n",
    "            loss = criterion(output1, output2, label)\n",
    "            running_loss += loss.item()\n",
    "    return running_loss / len(test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
